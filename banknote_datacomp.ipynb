{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Artificial noise upsampling for a comparison with GAN\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from numpy import ndarray,random\n",
    "import random\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to normalize data\n",
    "def normalize(x):\n",
    " \n",
    "    x=x.astype(float)\n",
    "    maxnorm= [np.amax(x[:,j])  if (j in range(x.shape[1]))==True else 1 for j in range(x.shape[1])]\n",
    "    minnorm= [np.amin(x[:,j])  if (j in range(x.shape[1]))== True else 0 for j in range(x.shape[1])]\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i][j]=(x[i][j]- minnorm[j])/(maxnorm[j]-minnorm[j])\n",
    "    return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract seismic data and preprocessing\n",
    "banknote=pd.read_csv('banknote_auth.csv')\n",
    "\n",
    "#input data\n",
    "X=banknote.iloc[:,0:4]\n",
    "\n",
    "X=np.asarray(X)\n",
    "\n",
    "#normalize data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X=preprocessing.scale(X)\n",
    "X=normalize(X)\n",
    "\n",
    "#training output\n",
    "y=banknote.iloc[:,4]\n",
    "y=np.asarray(y)\n",
    "y=np.ravel(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split (X_train contains both classes, but a stratified sampling is carried out)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,shuffle=True,random_state=np.random.randint(0,100),stratify=y)\n",
    "\n",
    "#extract class 1 data from training set\n",
    "hazardous = [X_train[i,:] for i in range(X_train.shape[0]) if y[i] ==1]\n",
    "hazardous=np.asarray(hazardous)\n",
    "\n",
    "\n",
    "#extract class 0 data from training set\n",
    "non_hazardous = [X_train[i,:] for i in range(X_train.shape[0]) if y[i] ==0]\n",
    "non_hazardous=np.asarray(non_hazardous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of all models used in this notebook\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import OrderedDict\n",
    "models = OrderedDict([\n",
    "#('Knn 2', KNeighborsClassifier(2)),\n",
    "#('Naive Bayes', GaussianNB()),\n",
    "#('Logistic Regression', LogisticRegression()),\n",
    "#('Classification Tree', DecisionTreeClassifier(max_depth=18)),\n",
    "#('Random Forest', RandomForestClassifier(max_depth=5, n_estimators=50)),\n",
    "('Multilayer Perceptron', MLPClassifier((4), activation='logistic',solver='adam', max_iter=100000,learning_rate_init=0.01, random_state=np.random.randint(0,100)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenates old data and generated one in a unique dataset;for the latter, \n",
    "def new_data(X_train,y_train,x):\n",
    "  \n",
    "    Xu_train=np.concatenate((X_train,x),axis=0)\n",
    "    yu_train=np.concatenate((y_train,np.ones(x.shape[0])),axis=0)\n",
    "       \n",
    "    return Xu_train,yu_train\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs bootstrap of a matrix of datapoints\n",
    "def bootstrap(x,boot_size,noise):\n",
    "    if boot_size==0:\n",
    "        return x\n",
    "    else:\n",
    "        s=np.random.randint(0,x.shape[0]-1,boot_size)\n",
    "        s=np.asarray(s)\n",
    "        if noise:\n",
    "            w=np.asarray([x[s[i],:] +np.random.uniform(-0.05,0.05) for i in range(s.shape[0])])\n",
    "        else:\n",
    "            w=np.asarray([x[s[i],:] for i in range(s.shape[0])])\n",
    "       \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "b_ord_of_mag=[10000,100000]\n",
    "u_ord_of_mag=[0,500,1000,2000,5000,10000,50000,100000]\n",
    "boot_size=3000\n",
    "\n",
    "\n",
    "hazards=np.asarray(hazardous)\n",
    "\n",
    "trials=10\n",
    "av_acc=0\n",
    "av_rec=0\n",
    "for i in range(trials):\n",
    "        np.random.seed(i)\n",
    "        #initialization\n",
    "        name = 'Multilayer Perceptron'\n",
    "        clf = models[name]\n",
    "        noise=True\n",
    "        hazards1= bootstrap(hazards,3000,noise)\n",
    "        for name, clf in models.items():\n",
    "            \n",
    "            Xu_train, yu_train= new_data(X_train,y_train,hazards1)\n",
    "            clf.fit(Xu_train, yu_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            y_pred=clf.predict(X_test)\n",
    "            rec_score= recall_score(y_test,y_pred)\n",
    "            print(score,rec_score,name,\" boot_size:\",boot_size)\n",
    "            av_acc=av_acc+score\n",
    "            av_rec=av_rec +rec_score\n",
    "av_acc=av_acc/trials\n",
    "av_rec=av_rec/trials\n",
    "print(\"average accuracy: \",av_acc,\"average_recall: \",av_rec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "data={'original_data':hazards,'generated data':hazards1}\n",
    "for key,value in data.items():\n",
    "    cov=np.cov(value.T)\n",
    "    print(cov.shape)\n",
    "    w,v=LA.eig(cov)\n",
    "    print(key,w[0:2])\n",
    "    print(v[:,0:3])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca=decomposition.PCA(n_components=2)\n",
    "\n",
    "\n",
    "data={'0 data ':non_hazardous ,'1 data ': hazards1}\n",
    "for key,value in data.items():\n",
    "    value_new=pca.fit_transform(value)\n",
    "    plt.scatter(value_new[:,0],value_new[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
