{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GAN oversampling for banknote authentication\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from numpy import ndarray,random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to normalize data\n",
    "\n",
    "def normalize(x):\n",
    "    \n",
    "    x=x.astype(float)\n",
    "    maxnorm= [np.amax(x[:,j])  if (j in range(x.shape[1]))==True else 1 for j in range(x.shape[1])]\n",
    "    minnorm= [np.amin(x[:,j])  if (j in range(x.shape[1]))== True else 0 for j in range(x.shape[1])]\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i][j]=(x[i][j]- minnorm[j])/(maxnorm[j]-minnorm[j])\n",
    "    return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract seismic data and preprocessing\n",
    "banknote=pd.read_csv('banknote_auth.csv')\n",
    "\n",
    "#input data\n",
    "X=banknote.iloc[:,0:4]\n",
    "X=np.asarray(X)\n",
    "\n",
    "#normalize data\n",
    "X=preprocessing.scale(X)\n",
    "X=normalize(X)\n",
    "\n",
    "#training output\n",
    "y=banknote.iloc[:,4]\n",
    "y=np.asarray(y)\n",
    "y=np.ravel(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x,y):\n",
    "    \n",
    "    y=np.reshape(y,(y.shape[0],1))\n",
    "    z=np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(z)\n",
    "    x1=z[:,0:-1]\n",
    "    y1=z[:,-1]\n",
    "    y1=np.ravel(y1)\n",
    "    return x1,y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split (X_train contains both classes, but a stratified sampling is carried out)\n",
    "X,y=shuffle(X,y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,shuffle=True,stratify=y, random_state=np.random.randint(0,100))\n",
    "\n",
    "#extract class 1 data from training set\n",
    "hazardous = [X_train[i,:] for i in range(X_train.shape[0]) if y[i] ==1]\n",
    "hazardous=np.asarray(hazardous)\n",
    "\n",
    "#extract class 0 data from training set\n",
    "non_hazardous = [X_train[i,:] for i in range(X_train.shape[0]) if y[i] ==0]\n",
    "non_hazardous=np.asarray(non_hazardous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of all models used in this notebook\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import OrderedDict\n",
    "models = OrderedDict([\n",
    "#('Knn 2', KNeighborsClassifier(2)),\n",
    "#('Naive Bayes', GaussianNB()),\n",
    "#('Logistic Regression', LogisticRegression()),\n",
    "#('Classification Tree', DecisionTreeClassifier(max_depth=18)),\n",
    "#('Random Forest', RandomForestClassifier(max_depth=5, n_estimators=50)),\n",
    "('Multilayer Perceptron', MLPClassifier((4), activation='logistic',solver='adam', max_iter=100000,learning_rate_init=0.01, random_state=np.random.randint(0,100)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs GAN oversampling with Tensorflow\n",
    "\n",
    "def GAN_ups(hazardous,name,ups_size):\n",
    "    \n",
    "    if ups_size==0:\n",
    "        return hazardous\n",
    "    \n",
    "    # Training Params\n",
    "    num_steps = hazardous.shape[0]\n",
    "    learning_rate = 1e-4\n",
    "    batch_size=1\n",
    "    # Network Params\n",
    "    image_dim = 4# \n",
    "    gen_hidden_dim =10\n",
    "    disc_hidden_dim =10\n",
    "    noise_dim = 4 # Noise data points\n",
    "    \n",
    "    \n",
    "    \n",
    "    # A custom initialization (see Xavier Glorot init)\n",
    "    def glorot_init(shape):\n",
    "        return tf.random_normal(shape=shape,seed=np.random.randint(0,100),stddev=1. / tf.sqrt(shape[0] / 2.))\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'gen_hidden1': tf.Variable(glorot_init([noise_dim, gen_hidden_dim])),\n",
    "        'gen_out': tf.Variable(glorot_init([gen_hidden_dim, image_dim])),\n",
    "        'disc_hidden1': tf.Variable(glorot_init([image_dim, disc_hidden_dim])),\n",
    "        'disc_out': tf.Variable(glorot_init([disc_hidden_dim, 1])),\n",
    "        'log_disc': tf.Variable(glorot_init([image_dim, 1]))\n",
    "    }\n",
    "    biases = {\n",
    "        'gen_hidden1': tf.Variable(tf.zeros([gen_hidden_dim])),\n",
    "        'gen_out': tf.Variable(tf.zeros([image_dim])),\n",
    "        'disc_hidden1': tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "        'disc_out': tf.Variable(tf.zeros([1])),\n",
    "        'log_disc':tf.Variable(tf.zeros([1]))\n",
    "    }\n",
    "   \n",
    "        \n",
    "   \n",
    "    \n",
    "    # Generator\n",
    "    def generator(x):\n",
    "        \n",
    "        hidden_layer = tf.matmul(x, weights['gen_hidden1'])\n",
    "        hidden_layer = tf.add(hidden_layer, biases['gen_hidden1'])\n",
    "        hidden_layer = tf.nn.relu(hidden_layer)\n",
    "        out_layer = tf.matmul(hidden_layer, weights['gen_out'])\n",
    "        out_layer = tf.add(out_layer, biases['gen_out'])\n",
    "        out_layer = tf.nn.sigmoid(out_layer)\n",
    "        return out_layer\n",
    "    \n",
    "  \n",
    "    # Discriminator\n",
    "    def discriminator(x,name):\n",
    "        \n",
    "        if name=='Multilayer Perceptron':\n",
    "            hidden_layer = tf.matmul(x, weights['disc_hidden1'])\n",
    "            hidden_layer = tf.add(hidden_layer, biases['disc_hidden1'])\n",
    "            hidden_layer = tf.nn.relu(hidden_layer)\n",
    "            out_layer = tf.matmul(hidden_layer, weights['disc_out'])\n",
    "            out_layer = tf.add(out_layer, biases['disc_out'])\n",
    "            out_layer = tf.nn.sigmoid(out_layer)\n",
    "            return out_layer\n",
    "        else:\n",
    "            out_layer = tf.matmul(x,weights['log_disc'])\n",
    "            out_layer = tf.add(out_layer, biases['log_disc'])\n",
    "            out_layer = tf.nn.sigmoid(out_layer)\n",
    "            return out_layer\n",
    "\n",
    "    # Build Networks\n",
    "    \n",
    "    # Network Inputs\n",
    "    gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "    disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input')\n",
    "    \n",
    "    \n",
    "    # Build Generator Network\n",
    "    gen_sample = generator(gen_input)\n",
    "    \n",
    "    # Build 2 Discriminator Networks (one from noise input, one from generated samples)\n",
    "    disc_real = discriminator(disc_input,name)\n",
    "    disc_fake = discriminator(gen_sample,name)\n",
    "\n",
    "    # Build Loss\n",
    "    gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
    "    disc_loss =-(tf.reduce_mean(tf.log(disc_real) + tf.log(1. - disc_fake)))\n",
    "\n",
    "    # Build Optimizers\n",
    "    optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    \n",
    "    # Training Variables for each optimizer\n",
    "    \n",
    "    # Generator Network Variables\n",
    "    gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
    "            biases['gen_hidden1'], biases['gen_out']]\n",
    "    # Discriminator Network Variables\n",
    "    if name == 'Multilayer Perceptron':\n",
    "        disc_vars = [weights['disc_hidden1'], weights['disc_out'],\n",
    "            biases['disc_hidden1'], biases['disc_out']]\n",
    "    else:\n",
    "        disc_vars = [ weights['log_disc'],biases['log_disc']]\n",
    "    \n",
    "\n",
    "    # Create training operations\n",
    "    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "    \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        i=0\n",
    "        for i in range(num_steps-batch_size):\n",
    "            # Prepare Data\n",
    "        \n",
    "            batch_x=hazardous[i:i+batch_size]\n",
    "        \n",
    "            # Generate noise to feed to the generator\n",
    "            z = np.random.uniform(-1, 1., size=[batch_size, noise_dim])\n",
    "\n",
    "            # Train\n",
    "            feed_dict = {disc_input: batch_x, gen_input: z}\n",
    "            if i % 1 == 0:\n",
    "                _,_, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],feed_dict=feed_dict)\n",
    "                if (i== num_steps-batch_size-2):\n",
    "                    print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "            else:\n",
    "                _, gl, dl = sess.run([train_disc, gen_loss, disc_loss],feed_dict=feed_dict)\n",
    "                if (i== num_steps-batch_size-2):\n",
    "                    print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #generate new data\n",
    "        z = np.random.uniform(-1, 1., size=[ups_size, noise_dim])\n",
    "        g = sess.run([gen_sample], feed_dict={gen_input: z})\n",
    "        g=np.asarray(g)\n",
    "        g=g.reshape(ups_size,image_dim)\n",
    "        sess.close()\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenates old data and generated one in a unique dataset;\n",
    "\n",
    "def new_data(X_train,y_train,x):\n",
    "    Xu_train=np.concatenate((X_train,x),axis=0)\n",
    "    yu_train=np.concatenate((y_train,np.ones(x.shape[0])),axis=0)\n",
    "       \n",
    "    return Xu_train,yu_train\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs bootstrap of a matrix of datapoints\n",
    "def bootstrap(x,boot_size):\n",
    "    if boot_size==0:\n",
    "        return x\n",
    "    else:\n",
    "        s=np.random.randint(0,x.shape[0]-1,boot_size)\n",
    "        s=np.asarray(s)\n",
    "        w=np.asarray([x[s[i],:] for i in range(s.shape[0])])\n",
    "        #x=np.concatenate((x,w),axis=0)\n",
    "        #x=np.asarray(x)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discretize some features\n",
    "def discretize(x):\n",
    "    bins=[np.arange(-0.5,2.5,1),np.arange(-0.5,3.5,1)/2.0,np.arange(-0.5,2.5,1),[],[],[],[],np.arange(-0.5,3.5,1)/2.0,\n",
    "          np.arange(-0.5,10.5,1)/9.0,np.arange(-0.5,9.5,1)/8.0,np.arange(-0.5,8.5,1)/7.0,np.arange(-0.5,4.5,1)/3.0]\n",
    "    \n",
    "    \n",
    "    my_range=[0,1,2,7,8,9,10,11]\n",
    "    for j in my_range:\n",
    "        bins_j=np.asarray(bins[j])\n",
    "        inds=np.digitize(x[:,j],bins_j)\n",
    "        inds=(inds-1)/(bins_j.shape[0]-2.0)\n",
    "        x[:,j]=[inds[i] for i in range(x.shape[0])]\n",
    " \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "def shuffle(x,y):\n",
    "    \n",
    "    y=np.reshape(y,(y.shape[0],1))\n",
    "    z=np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(z)\n",
    "    x1=z[:,0:-1]\n",
    "    y1=z[:,-1]\n",
    "    y1=np.ravel(y1)\n",
    "   \n",
    "    return x1,y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialization\n",
    "name = 'Multilayer Perceptron'\n",
    "clf = models[name]\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "b_ord_of_mag=[100000,300000,500000]\n",
    "boot_nh_mag=[1000,5000,10000,20000,50000]\n",
    "u_ord_of_mag=[1000,1500,2000,5000,10000]\n",
    "boot_size=100000\n",
    "ups_size=3000\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca=decomposition.PCA(n_components=4)\n",
    "\n",
    "hazards=np.asarray(hazardous)\n",
    "hazards= bootstrap(hazards,boot_size)\n",
    "#non_hazardous1=bootstrap(non_hazardous,1)\n",
    "\n",
    "trials=1\n",
    "av_acc=0\n",
    "av_rec=0\n",
    "for i in range(trials):\n",
    "    np.random.seed(i)\n",
    "    for name, clf in models.items():\n",
    "        X,y=shuffle(X,y)\n",
    "        hazard_GAN= GAN_ups(hazards,name,ups_size)\n",
    "        hazard_GAN=normalize(hazard_GAN)\n",
    "        #hazards1=hazards[0:1,:]\n",
    "        Xu_train, yu_train= new_data(X_train,y_train,hazard_GAN)\n",
    "        Xu_train,yu_train=shuffle(Xu_train,yu_train)\n",
    "        clf.fit(Xu_train, yu_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y_pred=clf.predict(X_test)\n",
    "        rec_score= recall_score(y_test,y_pred)\n",
    "        print(score,rec_score,name,\" boot_size:\",boot_size,\" ups_size:\",ups_size)\n",
    "        av_acc=av_acc+score\n",
    "        av_rec=av_rec +rec_score\n",
    "        data={'original_data':hazards,'gen_data':hazard_GAN}\n",
    "        for key,value in data.items():\n",
    "            #print(key, value.shape)\n",
    "            value_new=pca.fit_transform(value)\n",
    "            plt.scatter(value_new[:,0],value_new[:,1])\n",
    "\n",
    "        plt.show()\n",
    "av_acc=av_acc/trials\n",
    "av_rec=av_rec/trials\n",
    "print(\"average accuracy: \",av_acc,\"average_recall: \",av_rec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "data={'original_data':hazards,'generated data':hazard_GAN}\n",
    "for key,value in data.items():\n",
    "    cov=np.cov(value.T)\n",
    "    print(cov.shape)\n",
    "    w,v=LA.eig(cov)\n",
    "    print(key,w[0:2])\n",
    "    print(v[:,0:3])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "my_range=[0,2]\n",
    "for j in range(4):\n",
    "    z=hazards[:,j]\n",
    "    w=hazard_GAN[:,j]\n",
    "\n",
    "    plt.hist(z,normed=True,bins=100)\n",
    "    plt.hist(w,normed=True,bins=100)\n",
    "\n",
    "    plt.show()\n",
    "     \n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
